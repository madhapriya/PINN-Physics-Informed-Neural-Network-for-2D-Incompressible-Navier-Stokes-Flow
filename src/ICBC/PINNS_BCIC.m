% IC/BC
clearvars; clc;

% load simulation data
raw = load('cylinder_nektar_wake.mat');
X_star = raw.X_star;
t_star = raw.t;
U_star = raw.U_star;
P_star = raw.p_star;

N = size(X_star,1);
Tt = numel(t_star);

% make full coordinate and field vectors
x_full  = repmat(X_star(:,1), Tt, 1);
y_full  = repmat(X_star(:,2), Tt, 1);
t_full  = reshape(repmat(t_star', N, 1), N*Tt, 1);
u_full  = reshape(U_star(:,1,:), N*Tt, 1);
v_full  = reshape(U_star(:,2,:), N*Tt, 1);
p_full  = reshape(P_star,       N*Tt, 1);

% split into train and test before normalization
totalPoints = numel(x_full);
splitIdx = round(0.7 * totalPoints);
rng(0);
idxAll = randperm(totalPoints);

trainIdx = idxAll(1:splitIdx);
testIdx  = idxAll(splitIdx+1:end);

% training data (original scale)
x_train = x_full(trainIdx);
y_train = y_full(trainIdx);
t_train = t_full(trainIdx);
u_train = u_full(trainIdx);
v_train = v_full(trainIdx);
p_train = p_full(trainIdx);

% test data (original scale)
x_test = x_full(testIdx);
y_test = y_full(testIdx);
t_test = t_full(testIdx);
u_test = u_full(testIdx);
v_test = v_full(testIdx);
p_test = p_full(testIdx);

% find normalization values from training data
mu_in  = mean([x_train,y_train,t_train], 1);
s_in   = std( [x_train,y_train,t_train], 1);
mu_out = mean([u_train,v_train,p_train], 1);
s_out  = std( [u_train,v_train,p_train], 1);

inputNorm.mean = mu_in;
inputNorm.std  = s_in;
outputNorm.mean = mu_out;
outputNorm.std  = s_out;

% normalize training data
xN_train = (x_train - mu_in(1)) / s_in(1);
yN_train = (y_train - mu_in(2)) / s_in(2);
tN_train = (t_train - mu_in(3)) / s_in(3);
uN_train = (u_train - mu_out(1)) / s_out(1);
vN_train = (v_train - mu_out(2)) / s_out(2);
pN_train = (p_train - mu_out(3)) / s_out(3);

% keep copy of normalized training inputs for collocation range
trainInputsNorm = [xN_train, yN_train, tN_train];

% normalize test data with training values
xN_test = (x_test - mu_in(1)) / s_in(1);
yN_test = (y_test - mu_in(2)) / s_in(2);
tN_test = (t_test - mu_in(3)) / s_in(3);
uN_test = (u_test - mu_out(1)) / s_out(1);
vN_test = (v_test - mu_out(2)) / s_out(2);
pN_test = (p_test - mu_out(3)) / s_out(3);

% choose collocation points inside training range (normalized)
Nf = 20000;
minTrain = min(trainInputsNorm, [], 1);
maxTrain = max(trainInputsNorm, [], 1);
Xf_pool = minTrain + (maxTrain - minTrain) .* rand(Nf, 3);

% -------------------------
% Create IC and BC pools
% -------------------------
% Node indices in spatial mesh
node_idx = repmat((1:N)', Tt, 1);  % length N*Tt

% initial time t0 (exact match with t_star)
t0 = min(t_star);
% find indices in full data at initial time
ic_full_idx = find(abs(t_full - t0) < 1e-12); % tolerance
% now restrict to training indices
ic_train_mask = ismember(ic_full_idx, trainIdx);
ic_train_full_idx = ic_full_idx(ic_train_mask);  % indices (in full arrays) that are IC & in training

% Boundary spatial nodes: domain edges formed from X_star
minX = min(X_star(:,1)); maxX = max(X_star(:,1));
minY = min(X_star(:,2)); maxY = max(X_star(:,2));
spatialBoundaryNodes = find( abs(X_star(:,1) - minX) < 1e-12 | ...
                             abs(X_star(:,1) - maxX) < 1e-12 | ...
                             abs(X_star(:,2) - minY) < 1e-12 | ...
                             abs(X_star(:,2) - maxY) < 1e-12 );

% indices in full arrays corresponding to boundary spatial nodes (all times)
boundary_full_idx = find( ismember(node_idx, spatialBoundaryNodes) );
% restrict to training indices
boundary_train_mask = ismember(boundary_full_idx, trainIdx);
boundary_train_full_idx = boundary_full_idx(boundary_train_mask);

% Build normalized IC and BC pools (using training-only indices)
% Initial condition pool (normalized)
x_ic_pool = (x_full(ic_train_full_idx) - mu_in(1)) / s_in(1);
y_ic_pool = (y_full(ic_train_full_idx) - mu_in(2)) / s_in(2);
t_ic_pool = (t_full(ic_train_full_idx) - mu_in(3)) / s_in(3);
u_ic_pool = (u_full(ic_train_full_idx) - mu_out(1)) / s_out(1);
v_ic_pool = (v_full(ic_train_full_idx) - mu_out(2)) / s_out(2);
p_ic_pool = (p_full(ic_train_full_idx) - mu_out(3)) / s_out(3);
Xic_pool = [x_ic_pool, y_ic_pool, t_ic_pool];

% Boundary condition pool (normalized) -- could include time evolution at boundaries
x_bc_pool = (x_full(boundary_train_full_idx) - mu_in(1)) / s_in(1);
y_bc_pool = (y_full(boundary_train_full_idx) - mu_in(2)) / s_in(2);
t_bc_pool = (t_full(boundary_train_full_idx) - mu_in(3)) / s_in(3);
u_bc_pool = (u_full(boundary_train_full_idx) - mu_out(1)) / s_out(1);
v_bc_pool = (v_full(boundary_train_full_idx) - mu_out(2)) / s_out(2);
p_bc_pool = (p_full(boundary_train_full_idx) - mu_out(3)) / s_out(3);
Xbc_pool = [x_bc_pool, y_bc_pool, t_bc_pool];

% If pools are empty (unlikely) avoid errors
if isempty(Xic_pool)
    error('Initial-condition pool is empty. Check t_star/t_full matching.');
end
if isempty(Xbc_pool)
    warning('Boundary-condition pool is empty. No BC enforcement will be applied.');
end

% -------------------------
% make the neural network
% -------------------------
layers = [3, 80, 80, 80, 80, 80, 3];
lgraph = layerGraph(featureInputLayer(3,'Normalization','none','Name','in'));
for i = 2:length(layers)-1
    fcName = sprintf('fc%d', i-1);
    tanhName = sprintf('tanh%d', i-1);
    lgraph = addLayers(lgraph, [
        fullyConnectedLayer(layers(i),'Name',fcName)
        tanhLayer('Name',tanhName)]);
    if i == 2
        lgraph = connectLayers(lgraph,'in',fcName);
    else
        prevTanh = sprintf('tanh%d', i-2);
        lgraph = connectLayers(lgraph, prevTanh, fcName);
    end
end
lgraph = addLayers(lgraph, fullyConnectedLayer(3,'Name','out'));
lastTanh = sprintf('tanh%d', length(layers)-2);
lgraph = connectLayers(lgraph, lastTanh, 'out');
net = dlnetwork(lgraph);

% starting viscosity
viscosity = dlarray(0.01, 'CB');

% training with Adam
maxEpochs = 30;
miniBatchSize = 64;
learnRate = 1e-3;
trAvg = []; trAvgSq = [];
iteration = 0;

% hyperparameters for IC / BC enforcement
lambdaIC = 0.05;    % weight for initial condition loss
lambdaBC = 0.05;    % weight for boundary condition loss
Ni = min(128, size(Xic_pool,1));   % number of IC samples per batch
Nb = min(128, size(Xbc_pool,1));   % number of BC samples per batch

for epoch = 1:maxEpochs
    idx = randperm(splitIdx);
    xN_train = xN_train(idx); yN_train = yN_train(idx); tN_train = tN_train(idx);
    uN_train = uN_train(idx); vN_train = vN_train(idx); pN_train = pN_train(idx);

    % sample collocation subset for this epoch
    sub = randperm(Nf, 1000);
    Xf_epoch = Xf_pool(sub,:);

    % shuffle IC and BC pools for sampling in this epoch
    ic_perm = randperm(size(Xic_pool,1));
    bc_perm = randperm(size(Xbc_pool,1));
    % make repeated cycles if Ni or Nb > pool size
    ic_perm = ic_perm(mod(0:Ni-1, numel(ic_perm)) + 1);
    bc_perm = bc_perm(mod(0:Nb-1, numel(bc_perm)) + 1);
    % Not necessary to reshuffle per minibatch; we'll sample random indices per minibatch below.

    for i = 1:miniBatchSize:splitIdx
        iteration = iteration + 1;
        ib = i:min(i+miniBatchSize-1, splitIdx);

        dlX = dlarray([xN_train(ib)'; yN_train(ib)'; tN_train(ib)'], 'CB');
        dlU = dlarray(uN_train(ib)', 'CB');
        dlV = dlarray(vN_train(ib)', 'CB');
        dlP = dlarray(pN_train(ib)', 'CB');

        cf = randperm(1000, numel(ib));
        dlXf = dlarray(Xf_epoch(cf,:)', 'CB');

        % sample IC and BC for this minibatch
        ic_sel = randperm(size(Xic_pool,1), min(Ni, size(Xic_pool,1)));
        dlXic = dlarray(Xic_pool(ic_sel,:)', 'CB');
        dlUic = dlarray(u_ic_pool(ic_sel)', 'CB');
        dlVic = dlarray(v_ic_pool(ic_sel)', 'CB');
        dlPic = dlarray(p_ic_pool(ic_sel)', 'CB');

        bc_sel = randperm(size(Xbc_pool,1), min(Nb, size(Xbc_pool,1)));
        dlXbc = dlarray(Xbc_pool(bc_sel,:)', 'CB');
        dlUbc = dlarray(u_bc_pool(bc_sel)', 'CB');
        dlVbc = dlarray(v_bc_pool(bc_sel)', 'CB');
        dlPbc = dlarray(p_bc_pool(bc_sel)', 'CB');

        if canUseGPU
            dlX = gpuArray(dlX); dlU = gpuArray(dlU);
            dlV = gpuArray(dlV); dlP = gpuArray(dlP);
            dlXf = gpuArray(dlXf);
            dlXic = gpuArray(dlXic); dlUic = gpuArray(dlUic);
            dlVic = gpuArray(dlVic); dlPic = gpuArray(dlPic);
            dlXbc = gpuArray(dlXbc); dlUbc = gpuArray(dlUbc);
            dlVbc = gpuArray(dlVbc); dlPbc = gpuArray(dlPbc);
        end

        [lossData, lossPhys, lossIC, lossBC, resNorms, grads, ~, ~] = ...
            dlfeval(@modelGradients, net, viscosity, dlX, dlU, dlV, dlP, ...
                    dlXf, dlXic, dlUic, dlVic, dlPic, dlXbc, dlUbc, dlVbc, dlPbc, ...
                    inputNorm, outputNorm, lambdaIC, lambdaBC);

        % update network weights (Adam) and viscosity (simple gradient descent step)
        [net, trAvg, trAvgSq] = adamupdate(net, grads.net, trAvg, trAvgSq, iteration, learnRate);
        viscosity = viscosity - learnRate * grads.viscosity;

        if mod(iteration, 100) == 0
            nu_val = extractdata(viscosity);
            fprintf("Epoch %d, Iter %d - Loss_data=%.3e, Loss_phys=%.3e, Loss_IC=%.3e, Loss_BC=%.3e, nu=%.6f\n", ...
                epoch, iteration, lossData, lossPhys, lossIC, lossBC, nu_val);
        end
    end
end
fprintf("Adam training complete. Starting L-BFGS refinement.\n");

% L-BFGS using training data (pass full IC/BC pools and Xf_pool)
params0.net = net;
params0.viscosity = viscosity;
theta0 = double(gather(packNetworkAndViscosity(net, viscosity)));

opts = optimoptions('fminunc',...
    'Algorithm','quasi-newton',...
    'SpecifyObjectiveGradient',true,...
    'MaxIterations',30,...
    'Display','iter');

theta_opt = fminunc(@(theta)objectiveLBFGS(theta, params0, inputNorm, outputNorm, ...
                   xN_train, yN_train, tN_train, uN_train, vN_train, pN_train, ...
                   Xf_pool, Xic_pool, u_ic_pool, v_ic_pool, p_ic_pool, ...
                   Xbc_pool, u_bc_pool, v_bc_pool, p_bc_pool, lambdaIC, lambdaBC), ...
                   theta0, opts);

[net, viscosity] = unpackTheta(theta_opt, params0);
save('trained_PINN_LBFGS_withICBC_NEW.mat', ...
     'net', 'viscosity', 'inputNorm', 'outputNorm', ...
     'trainIdx', 'testIdx', 'X_star', 't_star');

% test evaluation
dlTestInput = dlarray([xN_test'; yN_test'; tN_test'], 'CB');
if canUseGPU
    dlTestInput = gpuArray(dlTestInput);
end
dlOutput = forward(net, dlTestInput);

uPred = extractdata(dlOutput(1,:))' * s_out(1) + mu_out(1);
vPred = extractdata(dlOutput(2,:))' * s_out(2) + mu_out(2);
pPred = extractdata(dlOutput(3,:))' * s_out(3) + mu_out(3);

uTrue = u_test;
vTrue = v_test;
pTrue = p_test;
R2u = 1 - sum((uTrue - uPred).^2) / sum((uTrue - mean(uTrue)).^2);
R2v = 1 - sum((vTrue - vPred).^2) / sum((vTrue - mean(vTrue)).^2);
R2p = 1 - sum((pTrue - pPred).^2) / sum((pTrue - mean(pTrue)).^2);
fprintf("Test RÂ² (u,v,p): %.4f, %.4f, %.4f\n", R2u, R2v, R2p);

finalNu = extractdata(viscosity);
fprintf("Final learned viscosity (nu): %.6f\n", finalNu);

% -------------------------
% function for gradients (now includes IC & BC enforcement)
% -------------------------
function [lossData, lossPhys, lossIC, lossBC, resNorms, grads, uN_pred, vN_pred] = ...
    modelGradients(net, viscosity, dlX, dlU, dlV, dlP, dlXf, ...
                   dlXic, dlUic, dlVic, dlPic, dlXbc, dlUbc, dlVbc, dlPbc, ...
                   inN, outN, lambdaIC, lambdaBC)

    % forward on training data (data loss)
    Y    = forward(net, dlX);
    uN_pred   = Y(1,:); 
    vN_pred   = Y(2,:); 
    pN_pred   = Y(3,:);
    lossData = mse(uN_pred, dlU) + mse(vN_pred, dlV) + 0.01*mse(pN_pred, dlP);

    % forward on collocation points for physics residuals
    Yf   = forward(net, dlXf);
    uNf  = Yf(1,:); vNf  = Yf(2,:); pNf  = Yf(3,:);
    muO  = outN.mean; sO = outN.std;
    muI  = inN.mean;  sI = inN.std;
    uf   = uNf * sO(1) + muO(1);
    vf   = vNf * sO(2) + muO(2);

    % derivatives in normalized coords
    gradU = dlgradient(sum(uNf,"all"), dlXf, 'EnableHigherDerivatives', true);
    u_xN = gradU(1,:);
    u_yN = gradU(2,:);
    u_tN = gradU(3,:);
    u_x = u_xN * (sO(1)/sI(1));
    u_y = u_yN * (sO(1)/sI(2));
    u_t = u_tN * (sO(1)/sI(3));

    gradV = dlgradient(sum(vNf,"all"), dlXf, 'EnableHigherDerivatives', true);
    v_xN = gradV(1,:);
    v_yN = gradV(2,:);
    v_tN = gradV(3,:);
    v_x = v_xN * (sO(2)/sI(1));
    v_y = v_yN * (sO(2)/sI(2));
    v_t = v_tN * (sO(2)/sI(3));

    gradP = dlgradient(sum(pNf,"all"), dlXf, 'EnableHigherDerivatives', true);
    p_x = gradP(1,:) * (sO(3)/sI(1));
    p_y = gradP(2,:) * (sO(3)/sI(2));

    gradU_x = dlgradient(sum(u_xN,"all"), dlXf, 'EnableHigherDerivatives', true);
    u_xx     = gradU_x(1,:) * (sO(1)/(sI(1)^2));

    gradU_y = dlgradient(sum(u_yN,"all"), dlXf, 'EnableHigherDerivatives', true);
    u_yy     = gradU_y(2,:) * (sO(1)/(sI(2)^2));

    gradV_x = dlgradient(sum(v_xN,"all"), dlXf, 'EnableHigherDerivatives', true);
    v_xx     = gradV_x(1,:) * (sO(2)/(sI(1)^2));

    gradV_y = dlgradient(sum(v_yN,"all"), dlXf, 'EnableHigherDerivatives', true);
    v_yy     = gradV_y(2,:) * (sO(2)/(sI(2)^2));

    nu    = viscosity;
    fu    = u_t + uf .* u_x + vf .* u_y + p_x - nu .* (u_xx + u_yy);
    fv    = v_t + uf .* v_x + vf .* v_y + p_y - nu .* (v_xx + v_yy);
    fc    = u_x + v_y;
    lossPhys = mean(fu.^2 + fv.^2 + fc.^2, 'all');
    resNorms = [sqrt(mean(fu.^2)), sqrt(mean(fv.^2)), sqrt(mean(fc.^2))];

    % ---- initial condition enforcement ----
    % forward on IC points (normalized outputs)
    Yic = forward(net, dlXic);
    uN_ic_pred = Yic(1,:);
    vN_ic_pred = Yic(2,:);
    pN_ic_pred = Yic(3,:);
    lossIC = mse(uN_ic_pred, dlUic) + mse(vN_ic_pred, dlVic) + 0.01*mse(pN_ic_pred, dlPic);

    % ---- boundary condition enforcement ----
    if ~isempty(dlXbc)
        Ybc = forward(net, dlXbc);
        uN_bc_pred = Ybc(1,:);
        vN_bc_pred = Ybc(2,:);
        pN_bc_pred = Ybc(3,:);
        lossBC = mse(uN_bc_pred, dlUbc) + mse(vN_bc_pred, dlVbc) + 0.01*mse(pN_bc_pred, dlPbc);
    else
        lossBC = dlarray(0.0);
    end

    % total loss: data + physics + IC + BC (with weights)
    lossTotal = lossData + 0.1 * lossPhys +lambdaIC * lossIC + lambdaBC * lossBC;

    % compute gradients
    [gradsNet, gradsVisc] = dlgradient(lossTotal, net.Learnables, viscosity);
    grads.net = gradsNet;
    grads.viscosity = gradsVisc;
end

% helper to pack parameters
function theta = packNetworkAndViscosity(net, viscosity)
    values = net.Learnables.Value;
    flatParams = cellfun(@(x) extractdata(x(:)), values, 'UniformOutput', false);
    flatParams = vertcat(flatParams{:});
    theta = [flatParams; extractdata(viscosity)];
end

% helper to unpack parameters
function [net, viscosity] = unpackTheta(theta, params0)
    layers = [3,80,80,80,80,80,3];
    lgraph = layerGraph(featureInputLayer(3,'Normalization','none','Name','in'));
    for i = 2:length(layers)-1
        fcName = sprintf('fc%d',i-1); tanhName = sprintf('tanh%d',i-1);
        lgraph = addLayers(lgraph, [fullyConnectedLayer(layers(i),'Name',fcName); tanhLayer('Name',tanhName)]);
        if i==2
            lgraph = connectLayers(lgraph,'in',fcName);
        else
            prevTanh = sprintf('tanh%d',i-2);
            lgraph = connectLayers(lgraph,prevTanh,fcName);
        end
    end
    lgraph = addLayers(lgraph, fullyConnectedLayer(3,'Name','out'));
    lastTanh = sprintf('tanh%d',length(layers)-2);
    lgraph = connectLayers(lgraph,lastTanh,'out');
    net = dlnetwork(lgraph);

    learnables = net.Learnables;
    idx = 1;
    for i = 1:height(learnables)
        sz = size(learnables.Value{i});
        n = prod(sz);
        newVal = reshape(theta(idx:idx+n-1), sz);
        learnables.Value{i} = dlarray(newVal);
        idx = idx + n;
    end
    net = dlnetwork(net.Layers);
    net.Learnables = learnables;

    viscosity = dlarray(theta(end), 'CB');
end

% objective for lbfgs (now receives IC/BC pools and lambda weights)
function [loss, grad] = objectiveLBFGS(theta, params0, inN, outN, x, y, t, u, v, p, ...
                                      Xf, Xic, u_ic, v_ic, p_ic, Xbc, u_bc, v_bc, p_bc, lambdaIC, lambdaBC)
    persistent callNum
    if isempty(callNum), callNum = 1; else callNum = callNum + 1; end

    [net, viscosity] = unpackTheta(theta, params0);
    dlX  = dlarray([x'; y'; t'], 'CB');
    dlU  = dlarray(u', 'CB');
    dlV  = dlarray(v', 'CB');
    dlP  = dlarray(p', 'CB');
    dlXf = dlarray(Xf', 'CB');

    % full IC and BC pools passed to objective
    dlXic = dlarray(Xic', 'CB');
    dlUic = dlarray(u_ic', 'CB');
    dlVic = dlarray(v_ic', 'CB');
    dlPic = dlarray(p_ic', 'CB');

    dlXbc = dlarray(Xbc', 'CB');
    dlUbc = dlarray(u_bc', 'CB');
    dlVbc = dlarray(v_bc', 'CB');
    dlPbc = dlarray(p_bc', 'CB');

    [lossData, lossPhys, lossIC, lossBC, resNorms, grads, ~, ~] = ...
        dlfeval(@modelGradients, net, viscosity, dlX, dlU, dlV, dlP, dlXf, ...
                dlXic, dlUic, dlVic, dlPic, dlXbc, dlUbc, dlVbc, dlPbc, ...
                inN, outN, lambdaIC, lambdaBC);
    lossTotal = lossData + 0.1 * lossPhys + lambdaIC * lossIC + lambdaBC * lossBC;

    % flatten gradients
    flatGrads = cellfun(@(x) extractdata(x(:)), grads.net.Value, 'UniformOutput', false);
    flatGrads = vertcat(flatGrads{:});
    grad = [flatGrads; extractdata(grads.viscosity)];

    loss = double(gather(extractdata(lossTotal)));
    grad = double(gather(grad));
    fprintf("L-BFGS Call %d - Loss=%.4e, Loss_data=%.4e, Loss_phys=%.4e, Loss_IC=%.4e, Loss_BC=%.4e, nu=%.5f\n", ...
            callNum, loss, double(lossData), double(lossPhys), double(lossIC), double(lossBC), extractdata(viscosity));
end
